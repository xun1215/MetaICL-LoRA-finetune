
# MetaICL 数据处理流程（整理版 Markdown）

> 本文将你的纸质笔记梳理为结构化文档：从 `load_data → HuggingFace Dataset → tensorize → 模型输入` 的全流程，并把关键数据结构、示例格式与易错点一并整理。

## 目录
- [1. 总览](#1-总览)
- [2. 数据流转路径](#2-数据流转路径)
- [3. 关键步骤与职责](#3-关键步骤与职责)
- [4. 数据结构说明](#4-数据结构说明)
- [5. 示例格式（is_first / no_first）](#5-示例格式is_first--no_first)
- [6. 编码与张量化（encode → tensorize）](#6-编码与张量化encode--tensorize)
- [7. 训练 vs 测试 的处理差异](#7-训练-vs-测试-的处理差异)
- [8. 常见坑位与检查清单](#8-常见坑位与检查清单)

## 1. 总览

- **目标**：把原始文本样本（含 `input / output / options`）组织为 **few-shot prompt**，再 **编码(tokenize)** 成模型可用的张量（`input_ids / attention_mask / labels`）。
- **核心概念**：
  - `dfs`：`{task_name: pandas.DataFrame}` 的字典
  - `sharded_inputs`：把每个 DataFrame 转为 `HuggingFace Dataset` 后的列表
  - `_tensorize_for_training(...)`：对单个 `Dataset` 做编码与拼接
  - `is_first`：示例样本（demonstrations）与正式样本的区分标记

## 2. 数据流转路径

```mermaid
flowchart LR
    load_data → dfs(dict of DataFrame)
    dfs(DataFrame) → sharded_inputs(Huggingface Dataset)
    tensorize_for_training(sharded_inputs)
    将 input 与 output 映射成 (x, y)
    encode(x, y) → metaicl_data
```

## 3. 关键步骤与职责

1. **`dfs = load_data(...)`**  
   - 读取配置与数据集  
   - 进行 k-shot 抽样（保证 `seed` 可复现）  
   - 产出：`{ task_name: DataFrame(input, output, options) }`

2. **DataFrame → Dataset**  
   - `Dataset.from_pandas(dfs[task])`  
   - 组织为 `sharded_inputs: list[Dataset]`，每个 `Dataset` 对应一个任务

3. **`tensorize_for_training(sharded_inputs)`**  
   - 遍历每个 `in_`（即单个 `Dataset`）  
   - 调 `_tensorize_for_training(in_)` 做实际拼接与编码  
   - 用 `inputs += out` **展平合并**（避免 `append` 造成嵌套）

4. **得到 `metaicl_data`**  
   - 统一格式的张量字典，可直接送入模型

## 4. 数据结构说明

### 4.1 `dfs`（字典）
```text
dfs: Dict[str, pandas.DataFrame]
{
  "amazon_polarity": DataFrame(columns=["input","output","options"]),
  "sst2":            DataFrame(columns=["input","output","options"]),
  ...
}
```

### 4.2 `sharded_inputs`
```text
sharded_inputs: List[Dataset]
[
  Dataset("amazon_polarity"),  # in_
  Dataset("sst2"),             # in_
  ...
]
```

- 单个样本结构：
```python
{
  "input": "text ...",
  "output": "positive",
  "options": ["positive", "negative"]
}
```

## 5. 示例格式（is_first / no_first）

### `is_first = True`（示范样本）
```
[input1]
[output1]
[option1]
[option2]
...
```

### `is_first = False`
做样本或训练
```
3*\n
[input]
[output]
```
做测试、推断：
```
3*\n
[input]
[option1]
[option2]
...
idx
```

## 6. 编码与张量化（encode → tensorize）

```python
def prepro_sentence_pair_single(self, ids1, ids2):
    if len(ids1) + len(ids2) + 1 > self.max_length:
        ids1 = ids1[len(ids1) + len(ids2) + 1 - self.max_length:]
    input_ids = ids1 + ids2 + [self.tokenizer.pad_token_id]
    attention_mask = [1] * len(input_ids) + [1]
    labels = [-100] * len(ids1) + ids2 + [self.tokenizer.pad_token_id]
    return input_ids, attention_mask, labels
```

## 7. 训练 vs 测试

| 项目 | 训练（train） | 测试（eval） |
|---|---|---|
| `attention_mask` | 标注有效 token，与训练/推断无关 | 同左 |
| `labels` | 计算 loss | 通常不传 labels |

## 8. 常见坑位

- 不要 `append`，要 `+=`（flatten）
- 处理 options 为空：
```python
np.all([opt=="" for opt in dp["options"]])
```


